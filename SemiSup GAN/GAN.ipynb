{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFtnX6oMPgOn"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "from keras.layers import Dense,Conv2D,Conv2DTranspose,Input,Reshape,Activation,Lambda\r\n",
        "from keras.layers.advanced_activations import LeakyReLU\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.layers import BatchNormalization,Dropout,Flatten\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential,Model\r\n",
        "import keras.backend as K\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "class Dataset:\r\n",
        "  '''\r\n",
        "  Class pour gérér les données\r\n",
        "  '''\r\n",
        "\r\n",
        "  def __init__(self,num_labeled,num_classes):\r\n",
        "    self.num_labeled = num_labeled\r\n",
        "    self.num_classes = num_classes\r\n",
        "    (self.x_train,self.y_train),(self.x_test,self.y_test) = mnist.load_data()      #loading mnist data\r\n",
        "  \r\n",
        "    def normalize_image(x):\r\n",
        "      '''\r\n",
        "      normaliser une image\r\n",
        "      '''\r\n",
        "      x = (x.astype(np.float32)-127.5)/127.5\r\n",
        "      x = np.expand_dims(x,axis=-1)\r\n",
        "      return x\r\n",
        "  \r\n",
        "    def reshape_labels(y):\r\n",
        "      return y.reshape((-1,1))\r\n",
        "\r\n",
        "    self.x_train = normalize_image(self.x_train)\r\n",
        "    self.y_train = reshape_labels(self.y_train)\r\n",
        "    self.x_test = normalize_image(self.x_test)\r\n",
        "    self.y_test = reshape_labels(self.y_test)\r\n",
        " \r\n",
        "  def sample_labeled(self,batch_size):\r\n",
        "    '''\r\n",
        "    générer des données annotées\r\n",
        "    '''\r\n",
        "    sample_per_class = int(self.num_labeled/self.num_classes)\r\n",
        "    imgs = list()\r\n",
        "    labels = list()\r\n",
        "\r\n",
        "    for i in range(self.num_classes):\r\n",
        "      idx = self.y_train==i                             \r\n",
        "      idx = [i for i, x in enumerate(idx) if x]\r\n",
        "      x_classes= self.x_train[idx]\r\n",
        "      y_classes = self.y_train[idx]\r\n",
        "\r\n",
        "      index = np.arange(sample_per_class)\r\n",
        "      x_imgs = x_classes[index]                \r\n",
        "      y_imgs = y_classes[index]\r\n",
        "      \r\n",
        "      [imgs.append(x_imgs[j]) for j in index ]\r\n",
        "      [labels.append(y_imgs[j]) for j in index]\r\n",
        " \r\n",
        "    imgs = np.asarray(imgs) \r\n",
        "    labels = np.asarray(labels)\r\n",
        " \r\n",
        "    return imgs,labels\r\n",
        " \r\n",
        "  def sample_unlabeled(self,batch_size):\r\n",
        "    '''\r\n",
        "    générer des images non étiquetées\r\n",
        "    '''\r\n",
        "    idx = np.random.randint(self.num_labeled,self.x_train.shape[0],batch_size)\r\n",
        "    imgs = self.x_train[idx]\r\n",
        "    return imgs\r\n",
        " \r\n",
        "  def get_training(self):\r\n",
        "      '''\r\n",
        "      récuperer les données d'apprentissage\r\n",
        "      '''\r\n",
        "      x_train = self.x_train[range(self.num_labeled)]  \r\n",
        "      y_train = self.y_train[range(self.num_labeled)]\r\n",
        "      return x_train ,y_train\r\n",
        "\r\n",
        "  def get_testing(self):\r\n",
        "    '''\r\n",
        "    récuperer les données de test\r\n",
        "    '''\r\n",
        "    return self.x_test,self.y_test\r\n",
        " \r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-INZDeFnPzOx"
      },
      "source": [
        "from keras.utils import to_categorical\r\n",
        "class GAN:\r\n",
        "  '''\r\n",
        "  Class pour gérer les modèles GAN\r\n",
        "  '''\r\n",
        "\r\n",
        "  def __init__(self,z_dim = 100,num_classes = 10):\r\n",
        "    '''\r\n",
        "    Créer tous les modèles\r\n",
        "    '''\r\n",
        "    img_rows = 28\r\n",
        "    img_cols = 28\r\n",
        "    channels = 1\r\n",
        "    \r\n",
        "    self.img_shape = (img_rows,img_cols,channels)\r\n",
        "\r\n",
        "    self.z_dim=z_dim\r\n",
        "    self.num_classes=num_classes\r\n",
        "    \r\n",
        "\r\n",
        "    self.discriminator = self.build_discriminator(self.img_shape)\r\n",
        "\r\n",
        "    self.discriminator_supervised = self.build_discriminator_supervised(self.discriminator)\r\n",
        "    self.discriminator_supervised.compile(optimizer= Adam(lr=0.0002, beta_1=0.5),loss=\"categorical_crossentropy\",metrics=['accuracy'])\r\n",
        "\r\n",
        "    self.discriminator_unsupervised = self.build_discriminator_unsupervised(self.discriminator)\r\n",
        "    self.discriminator_unsupervised.compile(optimizer = Adam(lr=0.0002, beta_1=0.5),loss='binary_crossentropy',metrics=['accuracy'])\r\n",
        "\r\n",
        "    self.generator = self.build_generator(z_dim)\r\n",
        "    \r\n",
        "    self.gan = self.build_gan(self.generator,self.discriminator_unsupervised)\r\n",
        "    self.gan.compile(optimizer=Adam(learning_rate=0.0002),loss='binary_crossentropy',metrics=['accuracy'])\r\n",
        "\r\n",
        "  def build_generator(self,z_dim):\r\n",
        "    '''\r\n",
        "    Créer le générateur\r\n",
        "    '''\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    model.add(Dense(128*7*7,input_dim=z_dim))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "    model.add(Reshape((7,7,128)))\r\n",
        "\r\n",
        "    model.add(Conv2DTranspose(128,kernel_size=4,strides=2,padding='same'))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    model.add(Conv2DTranspose(128,kernel_size=4,strides=2,padding='same'))\r\n",
        "    model.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "    model.add(Conv2D(1, (7,7), activation='tanh', padding='same'))\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "  def build_discriminator(self,img_shape):\r\n",
        "    '''\r\n",
        "    Créer le discriminateur\r\n",
        "    '''\r\n",
        "\r\n",
        "    inp = Input(shape=img_shape)\r\n",
        "\r\n",
        "    X = Conv2D(128,kernel_size=(3,3),strides=2,input_shape=img_shape,padding='same')(inp)\r\n",
        "    X = LeakyReLU(alpha=0.2)(X)\r\n",
        "\r\n",
        "    X = Conv2D(128,kernel_size=(3,3),strides=2,padding='same')(X)\r\n",
        "    X = LeakyReLU(alpha=0.2)(X)\r\n",
        "\r\n",
        "    X = Conv2D(128,kernel_size=(3,3),strides=2,padding='same')(X)\r\n",
        "    X = LeakyReLU(alpha=0.2)(X)\r\n",
        "\r\n",
        "    X = Flatten(name=\"flatten\")(X)\r\n",
        "    X = Dropout(0.4)(X)\r\n",
        "    X = Dense(num_classes)(X)\r\n",
        "    model = Model(inputs=inp,outputs=X)\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "  def build_discriminator_unsupervised(self,discriminator):\r\n",
        "    '''\r\n",
        "    créer la partie non supervisée du discriminateur\r\n",
        "    '''\r\n",
        "    model = Sequential()\r\n",
        "    model.add(discriminator)\r\n",
        "    def custom_activation(x):\r\n",
        "          \r\n",
        "      prediction = 1.0 - (1.0 /\r\n",
        "                            (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\r\n",
        "      return prediction\r\n",
        "\r\n",
        "    model.add(Lambda(custom_activation))\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "  def build_discriminator_supervised(self,discriminator):\r\n",
        "    '''\r\n",
        "    créer la partie supervisée du discriminateur\r\n",
        "    '''\r\n",
        "    model = Sequential()\r\n",
        "    model.add(discriminator)\r\n",
        "    model.add(Activation('softmax'))\r\n",
        "    return model\r\n",
        "\r\n",
        "  def build_gan(self,generator,discriminator):\r\n",
        "    '''\r\n",
        "    Concatener les deux modèles pour avoir un GAN.\r\n",
        "    '''\r\n",
        "    model = Sequential()\r\n",
        "    model.add(generator)\r\n",
        "    model.add(discriminator)\r\n",
        "    return model\r\n",
        "\r\n",
        "  def train(self,dataset,iterations= 12000,batch_size= 100,sample_interval= 500):\r\n",
        "    '''\r\n",
        "    Entraîner le modèle gan complet\r\n",
        "    '''\r\n",
        "\r\n",
        "    z_dim=self.z_dim\r\n",
        "\r\n",
        "    supervised_losses = []\r\n",
        "    iteration_checkpoints = []\r\n",
        "    accuracies = []\r\n",
        "    val_losses = []\r\n",
        "    val_accuracies=[]\r\n",
        "\r\n",
        "    half = int(batch_size/2)\r\n",
        "    real = np.ones((half,1))\r\n",
        "    full_real=np.ones((batch_size,1))\r\n",
        "    fake = np.zeros((half,1))\r\n",
        "\r\n",
        "    x_test,y_test=dataset.get_testing()\r\n",
        "    y_test=to_categorical(y_test,num_classes=num_classes)\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "    for iteration in range(iterations):\r\n",
        "      imgs,labels = dataset.sample_labeled(half)\r\n",
        "      labels = to_categorical(labels,num_classes=self.num_classes)\r\n",
        "      unlabeled_imgs = dataset.sample_unlabeled(half)\r\n",
        "\r\n",
        "      z = np.random.normal(0,1,(half,z_dim))\r\n",
        "      \r\n",
        "      fake_imgs = self.generator.predict(z)\r\n",
        "\r\n",
        "      d_supervised_loss,accuracy = self.discriminator_supervised.train_on_batch(imgs,labels)\r\n",
        "      d_unsupervised_loss_real = self.discriminator_unsupervised.train_on_batch(unlabeled_imgs,real)\r\n",
        "      d_unsupervised_loss_fake = self.discriminator_unsupervised.train_on_batch(fake_imgs,fake)\r\n",
        "      d_unsupervised_loss = 0.5*np.add(d_unsupervised_loss_real,d_unsupervised_loss_fake)\r\n",
        "\r\n",
        "      z = np.random.normal(0,1,(batch_size,z_dim))\r\n",
        "      fake_imgs = self.generator.predict(z)\r\n",
        "      generator_loss = self.gan.train_on_batch(z,full_real)\r\n",
        "\r\n",
        "      \r\n",
        "      if (iteration+1) % sample_interval ==0:\r\n",
        "        supervised_losses.append(d_supervised_loss)\r\n",
        "        accuracies.append(100*accuracy)\r\n",
        "        iteration_checkpoints.append(iteration+1)\r\n",
        "        val_loss = self.discriminator_supervised.evaluate(x=x_test,y=y_test,verbose=0)\r\n",
        "        val_losses.append(val_loss[0])\r\n",
        "        val_accuracies.append(val_loss[1])\r\n",
        "        print(\"Iteration No.:\",iteration+1,end=\",\")\r\n",
        "        print(\"Discriminator Supervised Loss:\",d_supervised_loss,end=',')\r\n",
        "        print('Generator Loss:',generator_loss,end=\",\")\r\n",
        "        print('Discriminator Unsuperived Loss:',d_unsupervised_loss,sep=',')\r\n",
        "        print('val_loss:',val_loss,sep=',')\r\n",
        "        print('Accuracy Supervised:',100*accuracy)\r\n",
        "\r\n",
        "        ''' filename1 = 'generator_%04d.h5' % (iteration+1)\r\n",
        "        self.generator.save(filename1)\r\n",
        "        # save the classifier model\r\n",
        "        filename2 = 'classifier_%04d.h5' % (iteration+1)\r\n",
        "        self.discriminator_supervised.save(filename2) '''"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWf9FHpEPyZr"
      },
      "source": [
        "'''\r\n",
        "Créer le dataset et lancer l'entraînement\r\n",
        "'''\r\n",
        "num_labeled = 100\r\n",
        "num_classes = 10\r\n",
        "\r\n",
        "dataset = Dataset(num_labeled,num_classes)\r\n",
        "gan=GAN()\r\n",
        "\r\n",
        "gan.train(dataset=dataset,iterations=200,sample_interval=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUgYyFK0QgJU"
      },
      "source": [
        "'''\r\n",
        "Tester le modèle après entraînement\r\n",
        "'''\r\n",
        "\r\n",
        "x_test,y_test=dataset.get_testing()\r\n",
        "y_test=to_categorical(y_test,num_classes=num_classes)\r\n",
        "loss,accuracy=gan.discriminator_supervised.evaluate(x=x_test,y=y_test,verbose=0)\r\n",
        "print(loss,accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_3InulxSWls"
      },
      "source": [
        "'''\r\n",
        "Vérifier la qualité des images par le générateur\r\n",
        "'''\r\n",
        "\r\n",
        "n_images,zdim=100,100\r\n",
        "z = np.random.normal(0,1,(n_images,zdim))\r\n",
        "gen_imgs=gan.generator.predict(z)\r\n",
        "\r\n",
        "M = 10\r\n",
        "fig, axs = plt.subplots(M, M, figsize=(20,20))\r\n",
        "\r\n",
        "for i in range(M):\r\n",
        "    for j in range(M):\r\n",
        "        axs[i,j].imshow(gen_imgs[10*i+j,:,:,0],cmap='gray')\r\n",
        "        axs[i,j].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}